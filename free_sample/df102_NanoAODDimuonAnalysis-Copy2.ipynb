{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROOT Logo](http://root.cern.ch/img/logos/ROOT_Logo/website-banner/website-banner-%28not%20root%20picture%29.jpg)\n",
    "<br />\n",
    "# **NanoAOD files processed with Distristruted RDataFrame in Python**\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">\n",
    "\n",
    "\n",
    "[`df102_NanoAODDimuonAnalysis`](https://root.cern.ch/doc/master/df102__NanoAODDimuonAnalysis_8py.html) ROOT tutorial running with PyRDF.\n",
    "\n",
    "The NanoAOD-like input files are filled with 66 mio. events from CMS OpenData containing muon candidates part of 2012 dataset ([DOI: 10.7483/OPENDATA.CMS.YLIC.86ZZ](http://opendata.cern.ch/record/6004) and [DOI: 10.7483/OPENDATA.CMS.M5AD.Y3V3](http://opendata.cern.ch/record/6030)).\n",
    "\n",
    "The macro matches muon pairs and produces an histogram of the dimuon mass spectrum showing resonances up to the Z mass. Note that the bump at 30 GeV is not a resonance but a trigger effect.\n",
    "\n",
    "Some more details about the dataset:\n",
    "- It contains about 66 millions events (muon and electron collections, plus some other information, e.g. about primary vertices)\n",
    "- It spans two compressed ROOT files located on EOS for about a total size of 7.5 GB.\n",
    "\n",
    "Date: April 2019<br>\n",
    "Author: Stefan Wunsch (KIT, CERN)<br>\n",
    "Adapted to PyRDF: Javier Cervantes Villanueva (CERN)\n",
    "\n",
    "**Requirements: ROOT-HEAD (Use the Bleeding Edge in the SWAN configuration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.AWS.RDataFrame\n",
    "\n",
    "\n",
    "def run_cpubound(npartitions=10):\n",
    "    # Create the RDF\n",
    "    # Increasing nentries would increase the overall runtime\n",
    "    nentries = int(1e9)\n",
    "    df = RDataFrame(nentries, npartitions=npartitions)\n",
    "\n",
    "    # Decide parameters of the random distributions of the RDF columns\n",
    "    gaus_mean = 10\n",
    "    gaus_sigma = 1\n",
    "    exp_tau = 20\n",
    "    poisson_mean = 30\n",
    "\n",
    "    df_withcols = df.Define(\"x\",f\"gRandom->Gaus({gaus_mean},{gaus_sigma})\")\\\n",
    "                    .Define(\"y\",f\"gRandom->Exp({exp_tau})\")\\\n",
    "                    .Define(\"z\",f\"gRandom->PoissonD({poisson_mean})\")\n",
    "\n",
    "    # Decide how many operations per column you want to run\n",
    "    # Increasing this would increase the overall runtime\n",
    "    nops_percol = 10\n",
    "    oplist = [df_withcols.Mean(f\"{colname}\") for colname in [\"x\",\"y\",\"z\"] for _ in range(nops_percol)]\n",
    "\n",
    "    # Start a stopwatch and trigger the execution of the computation graph.\n",
    "    # Asking for the first value in the list is enough to trigger everything\n",
    "    print(\"Starting the CPU bound benchmark.\")\n",
    "    t = ROOT.TStopwatch()\n",
    "    first_value = oplist[0].GetValue()\n",
    "    realtime = round(t.RealTime(), 2)\n",
    "    print(f\"CPU bound benchmark finished in {realtime} seconds.\")\n",
    "\n",
    "    # Decide the name of the output csv to store runtime information.\n",
    "    outcsv = \"distrdf_cpubound.csv\"\n",
    "\n",
    "    with open(outcsv, \"a+\") as f:\n",
    "        f.write(str(realtime))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# run_cpubound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the CPU bound benchmark.\n",
      "Benchmark report: AWSBENCH(npartitions=160, mapwalltime=198.0167, reducewalltime=2.2468)\n",
      "CPU bound benchmark finished in 204.05 seconds.\n",
      "Starting the CPU bound benchmark.\n",
      "Benchmark report: AWSBENCH(npartitions=80, mapwalltime=203.5296, reducewalltime=0.9778)\n",
      "CPU bound benchmark finished in 207.16 seconds.\n",
      "Starting the CPU bound benchmark.\n",
      "Benchmark report: AWSBENCH(npartitions=40, mapwalltime=219.1495, reducewalltime=0.4676)\n",
      "CPU bound benchmark finished in 222.42 seconds.\n",
      "Starting the CPU bound benchmark.\n",
      "Benchmark report: AWSBENCH(npartitions=20, mapwalltime=253.9883, reducewalltime=0.666)\n",
      "CPU bound benchmark finished in 257.27 seconds.\n",
      "Starting the CPU bound benchmark.\n",
      "Benchmark report: AWSBENCH(npartitions=10, mapwalltime=317.3827, reducewalltime=0.1062)\n",
      "CPU bound benchmark finished in 320.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "for partition in range(2,7):\n",
    "    run_cpubound(10*2**(6-partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.executor.extraLibraryPath",
     "value": "{LD_LIBRARY_PATH},PyRDF"
    },
    {
     "name": "spark.yarn.dist.archives",
     "value": "PyRDF.zip#PyRDF"
    },
    {
     "name": "spark.yarn.dist.files",
     "value": "./initialize.h"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
